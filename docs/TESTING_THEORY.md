# üß™ **Teoria Testowania Frameworka Mancer**

## üìã **Spis tre≈õci**
1. [Wprowadzenie do testowania framework√≥w](#wprowadzenie)
2. [Specyfika testowania bash wrapper](#bash-wrapper)
3. [Piramida test√≥w dla Mancer](#piramida)
4. [Strategie testowania w Docker](#docker-testing)
5. [Testy automatyczne vs manualne](#automatyzacja)
6. [Architektura systemu test√≥w](#architektura)
7. [Best practices](#best-practices)
8. [Plan implementacji](#plan)

---

## üéØ **1. Wprowadzenie do testowania framework√≥w** {#wprowadzenie}

### **Dlaczego testowanie frameworka jest inne ni≈º aplikacji?**

```mermaid
graph TD
    A[Framework Mancer] --> B[Dostarcza API/Interface]
    A --> C[ZarzƒÖdza wykonaniem komend]
    A --> D[Abstrakcja nad bash]
    A --> E[Obs≈Çuguje r√≥≈ºne backends]
    
    B --> F[Testy API]
    C --> G[Testy wykonania]
    D --> H[Testy integracji bash]
    E --> I[Testy backend switching]
```

### **Co framework Mancer MUSI testowaƒá:**

1. **üîß Funkcjonalno≈õƒá podstawowa**
   - Czy CommandFactory tworzy komendy poprawnie?
   - Czy BashBackend wykonuje bash commands?
   - Czy ShellRunner orchestruje wszystko razem?

2. **üîó Integracja komponent√≥w**
   - Czy ShellRunner + CommandFactory + BashBackend dzia≈ÇajƒÖ razem?
   - Czy context management dzia≈Ça?
   - Czy error handling jest konsystentny?

3. **‚ö° Wydajno≈õƒá i stabilno≈õƒá**
   - Czy cache dzia≈Ça poprawnie?
   - Czy nie ma memory leaks?
   - Czy obs≈Çuguje concurrent execution?

4. **üåê Kompatybilno≈õƒá ≈õrodowisk**
   - Czy dzia≈Ça w r√≥≈ºnych systemach?
   - Czy SSH backend dzia≈Ça tak samo jak bash?
   - Czy konteneryzacja nie psuje funkcjonalno≈õci?

---

## üõ†Ô∏è **2. Specyfika testowania bash wrapper** {#bash-wrapper}

### **Framework Mancer = Bash Wrapper + Abstrakcja**

```python
# To co testujemy:
runner = ShellRunner(backend_type="bash")
echo_cmd = runner.create_command("echo").text("test")
result = runner.execute(echo_cmd)

# Sprawdzamy:
assert result.success == True           # ‚Üê Framework dzia≈Çanie
assert "test" in result.raw_output     # ‚Üê Bash execution
assert result.exit_code == 0           # ‚Üê Command result
```

### **3 warstwy testowania bash wrapper:**

#### **Warstwa 1: Unit Tests (Izolowane komponenty)**
```python
class TestCommandFactory:
    def test_create_echo_command(self):
        factory = CommandFactory("bash")
        cmd = factory.create_command("echo")
        assert cmd is not None
        assert hasattr(cmd, 'text')
        assert hasattr(cmd, 'build_command')
```

#### **Warstwa 2: Integration Tests (Komponenty + bash)**
```python
class TestFrameworkIntegration:
    def test_echo_command_execution(self):
        runner = ShellRunner(backend_type="bash")
        echo_cmd = runner.create_command("echo").text("framework test")
        result = runner.execute(echo_cmd)
        
        assert result.success == True
        assert "framework test" in result.raw_output
```

#### **Warstwa 3: E2E Tests (Framework + Docker + Environment)**
```python
class TestDockerIntegration:
    def test_framework_in_container(self):
        # Test frameworka w izolowanym ≈õrodowisku Docker
        container_result = execute_in_container(
            "python3 -c 'from mancer import ShellRunner; print(\"OK\")'"
        )
        assert "OK" in container_result.stdout
```

---

## üèóÔ∏è **3. Piramida test√≥w dla Mancer** {#piramida}

```
                    üî∫ E2E Tests (Docker Integration)
                   /     - Framework w kontenerach
                  /      - SSH connectivity  
                 /       - Network communication
                /        - Performance metrics
               /         
              /        üî∫ Integration Tests
             /         - ShellRunner + BashBackend
            /          - CommandFactory + Commands
           /           - Cache functionality
          /            - Context management
         /
        /           üî∫üî∫üî∫ Unit Tests 
       /            - CommandFactory
      /             - BashBackend  
     /              - Individual Commands
    /               - Command building
   /                - Error handling
  /
 /________________üî∫üî∫üî∫üî∫ Static Analysis
                  - Code linting
                  - Type checking
                  - Security scans
```

### **Proporcje test√≥w (Test Pyramid):**
- **70%** Unit Tests - szybkie, izolowane, mockowane
- **20%** Integration Tests - komponenty razem, prawdziwy bash
- **10%** E2E Tests - framework w Docker, pe≈Çna integracja

### **Dlaczego ta proporcja?**

**Unit Tests (70%)** - Bo framework ma du≈ºo logiki:
- Command building logic
- Parameter validation
- Error handling
- Cache management
- Context switching

**Integration Tests (20%)** - Bo bash integration jest kluczowy:
- Prawdziwe wykonanie bash commands
- Output parsing
- Error code handling
- Environment variables

**E2E Tests (10%)** - Bo Docker/SSH to dodatkowa z≈Ço≈ºono≈õƒá:
- Najwolniejsze
- Najbardziej flaky
- Ale sprawdzajƒÖ rzeczywiste u≈ºycie

---

## üê≥ **4. Strategie testowania w Docker** {#docker-testing}

### **Dlaczego Docker dla test√≥w frameworka?**

1. **üîí Izolacja ≈õrodowiska**
   ```bash
   # Framework mo≈ºe mieƒá side effects
   runner.execute(runner.create_command("rm").file("/tmp/test"))
   # Docker izoluje to od host system
   ```

2. **üåê Standaryzacja ≈õrodowiska**
   ```yaml
   # Ka≈ºdy test ma identyczne ≈õrodowisko
   containers:
     mancer-test-1: { image: ubuntu:22.04, python: 3.10 }
     mancer-test-2: { image: ubuntu:22.04, python: 3.10 }
   ```

3. **üîó Test komunikacji SSH**
   ```python
   # Framework musi dzia≈Çaƒá przez SSH
   runner.set_remote_execution(host="mancer-test-2", user="mancer2")
   result = runner.execute(runner.create_command("hostname"))
   assert "mancer-test-2" in result.raw_output
   ```

### **Strategia Docker test√≥w:**

#### **A) Statyczne kontenery (mancer-test-1, 2, 3)**
```python
@pytest.fixture(scope="session")
def docker_containers():
    # Kontenery startujƒÖ raz na sesjƒô test√≥w
    # Szybsze dla wielu test√≥w
    return ["mancer-test-1", "mancer-test-2", "mancer-test-3"]
```

#### **B) Dynamiczne kontenery (per test)**
```python
@pytest.fixture(scope="function")  
def clean_container():
    # Nowy kontener na ka≈ºdy test
    # Wolniejsze, ale izolowane
    container = docker.create_container("ubuntu:22.04")
    yield container
    docker.remove_container(container)
```

#### **C) Hybrid approach (Nasz wyb√≥r)**
```python
# Statyczne kontenery dla integration tests
# Dynamiczne dla E2E kt√≥re potrzebujƒÖ clean state
```

---

## ü§ñ **5. Testy automatyczne vs manualne** {#automatyzacja}

### **Co MUSI byƒá automatyczne:**

#### **‚úÖ Regression Tests**
```python
def test_ls_command_backward_compatibility():
    # Ka≈ºda zmiana w kodzie musi przej≈õƒá ten test
    runner = ShellRunner(backend_type="bash")
    ls_cmd = runner.create_command("ls")
    result = runner.execute(ls_cmd)
    
    assert result.success == True  # ‚Üê Nie mo≈ºe siƒô zepsuƒá
    assert hasattr(result, 'raw_output')
    assert hasattr(result, 'exit_code')
```

#### **‚úÖ CI/CD Pipeline Tests**
```yaml
# .github/workflows/test.yml
name: Framework Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Unit Tests
        run: pytest tests/unit/ -v
      - name: Integration Tests  
        run: docker-compose up -d && pytest tests/integration/
      - name: Performance Tests
        run: python3 tests/performance/benchmark.py
```

#### **‚úÖ Smoke Tests (Szybka weryfikacja)**
```python
def test_framework_smoke():
    """Czy framework w og√≥le startuje?"""
    runner = ShellRunner(backend_type="bash")
    assert runner is not None
    assert runner.factory is not None
    
    # Czy podstawowa komenda dzia≈Ça?
    echo_cmd = runner.create_command("echo").text("smoke test")
    result = runner.execute(echo_cmd)
    assert result.success == True
```

### **Co mo≈ºe byƒá manualne:**

#### **üîç Exploratory Testing**
- Testowanie edge cases kt√≥rych nie przewidzieli≈õmy
- Performance testing w r√≥≈ºnych ≈õrodowiskach
- Usability testing API frameworka

#### **üêõ Bug Reproduction**
```python
def test_bug_issue_123():
    """Manual test case for bug found in production"""
    # Reprodukuje konkretny bug zg≈Çoszony przez u≈ºytkownika
    runner = ShellRunner(backend_type="bash")
    # ... specific case that was failing
```

---

## üèõÔ∏è **6. Architektura systemu test√≥w** {#architektura}

### **Struktura katalog√≥w:**

```
tests/
‚îú‚îÄ‚îÄ unit/                    # 70% test√≥w - szybkie, mockowane
‚îÇ   ‚îú‚îÄ‚îÄ test_commands.py     # CommandFactory + poszczeg√≥lne komendy
‚îÇ   ‚îú‚îÄ‚îÄ test_chain.py        # ShellRunner + command chains
‚îÇ   ‚îú‚îÄ‚îÄ test_backend.py      # BashBackend + SSH backend
‚îÇ   ‚îî‚îÄ‚îÄ test_cache.py        # Cache functionality
‚îÇ
‚îú‚îÄ‚îÄ integration/             # 20% test√≥w - komponenty + bash
‚îÇ   ‚îú‚îÄ‚îÄ test_bash_commands.py    # Framework + prawdziwy bash
‚îÇ   ‚îú‚îÄ‚îÄ test_ssh_integration.py  # Framework + SSH
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py            # Utilities dla test√≥w
‚îÇ
‚îú‚îÄ‚îÄ e2e/                     # 10% test√≥w - framework + Docker
‚îÇ   ‚îú‚îÄ‚îÄ test_docker_integration.py  # Framework w kontenerach
‚îÇ   ‚îú‚îÄ‚îÄ test_performance.py         # Performance w Docker
‚îÇ   ‚îî‚îÄ‚îÄ test_network.py             # SSH miƒôdzy kontenerami
‚îÇ
‚îú‚îÄ‚îÄ fixtures/                # Test data
‚îÇ   ‚îú‚îÄ‚îÄ mock_responses/      # Mock outputs dla bash commands
‚îÇ   ‚îî‚îÄ‚îÄ test_configs/        # Test configurations
‚îÇ
‚îî‚îÄ‚îÄ conftest.py             # Pytest configuration
```

### **Test Utilities Architecture:**

```python
# tests/utils/framework_test_utils.py
class MancerTestUtils:
    """Centralized utilities dla wszystkich test√≥w"""
    
    @staticmethod
    def create_mock_runner() -> ShellRunner:
        """Tworzy ShellRunner z mockowanym backend"""
        
    @staticmethod  
    def execute_in_docker(container: str, command: str):
        """Wykonuje komendƒô w kontenerze Docker"""
        
    @staticmethod
    def validate_command_result(result: CommandResult):
        """Standardowa walidacja wyniku komendy"""
        
    @staticmethod
    def wait_for_container_ready(container: str, timeout: int):
        """Czeka a≈º kontener bƒôdzie gotowy do test√≥w"""
```

---

## üéØ **7. Best practices** {#best-practices}

### **A) Test Naming Convention**

```python
# ‚ùå ≈πle
def test_command():
    pass

# ‚úÖ Dobrze - opisuje co testuje
def test_echo_command_creates_valid_bash_string():
    pass
    
def test_shell_runner_executes_echo_command_successfully():
    pass

def test_bash_backend_handles_command_timeout_gracefully():
    pass
```

### **B) Test Data Management**

```python
# ‚ùå ≈πle - hardcoded values w testach
def test_echo():
    result = runner.execute(runner.create_command("echo").text("some random text"))
    assert "some random text" in result.raw_output

# ‚úÖ Dobrze - parametrized i reusable
@pytest.mark.parametrize("test_text", [
    "simple text",
    "text with spaces", 
    "text'with'quotes",
    "text\"with\"double_quotes",
    "text\nwith\nnewlines"
])
def test_echo_command_handles_various_inputs(test_text):
    result = runner.execute(runner.create_command("echo").text(test_text))
    assert test_text in result.raw_output
```

### **C) Error Testing Strategy**

```python
class TestFrameworkErrorHandling:
    def test_invalid_command_returns_error_result(self):
        """Framework powinien gracefully handle invalid commands"""
        runner = ShellRunner(backend_type="bash")
        invalid_cmd = runner.create_command("nonexistent_command_12345")
        
        if invalid_cmd is None:
            # Expected behavior - factory returns None
            assert True
        else:
            # Je≈õli factory co≈õ zwr√≥ci, to execution powinno fail gracefully
            result = runner.execute(invalid_cmd)
            assert result.success == False
            assert result.exit_code != 0
    
    def test_timeout_handling(self):
        """Framework powinien handle timeouts"""
        runner = ShellRunner(backend_type="bash", timeout=1)
        # Komenda kt√≥ra trwa d≈Çugo
        sleep_cmd = runner.create_command("sleep").seconds(10)
        result = runner.execute(sleep_cmd)
        
        assert result.success == False
        assert "timeout" in result.error_output.lower()
```

### **D) Performance Testing**

```python
class TestFrameworkPerformance:
    def test_command_execution_time(self):
        """Framework nie powinien dodawaƒá du≈ºego overhead"""
        runner = ShellRunner(backend_type="bash")
        
        start_time = time.time()
        result = runner.execute(runner.create_command("echo").text("perf test"))
        execution_time = time.time() - start_time
        
        assert result.success == True
        assert execution_time < 0.1  # Framework overhead < 100ms
    
    @pytest.mark.parametrize("cache_enabled", [True, False])
    def test_cache_performance_impact(self, cache_enabled):
        """Cache powinien przyspieszaƒá powtarzalne komendy"""
        runner = ShellRunner(
            backend_type="bash", 
            enable_cache=cache_enabled,
            cache_size=100
        )
        
        cmd = runner.create_command("echo").text("cache test")
        
        # Pierwsze wykonanie
        start = time.time()
        result1 = runner.execute(cmd)
        first_time = time.time() - start
        
        # Drugie wykonanie (z cache je≈õli enabled)
        start = time.time()  
        result2 = runner.execute(cmd)
        second_time = time.time() - start
        
        assert result1.success == True
        assert result2.success == True
        
        if cache_enabled:
            # Z cache powinno byƒá szybsze
            assert second_time < first_time
```

---

## üìù **8. Plan implementacji** {#plan}

### **Faza 1: Foundation (Tydzie≈Ñ 1-2)**

```python
# 1. Podstawowe unit testy
tests/unit/
‚îú‚îÄ‚îÄ test_command_factory.py     # CommandFactory core logic
‚îú‚îÄ‚îÄ test_bash_backend.py        # BashBackend execution
‚îî‚îÄ‚îÄ test_shell_runner.py        # ShellRunner orchestration

# 2. Test utilities
tests/utils/
‚îî‚îÄ‚îÄ framework_test_utils.py     # Reusable test utilities

# 3. CI/CD integration
.github/workflows/
‚îî‚îÄ‚îÄ tests.yml                   # Automated testing pipeline
```

### **Faza 2: Integration (Tydzie≈Ñ 3)**

```python
# 1. Integration testy
tests/integration/
‚îú‚îÄ‚îÄ test_framework_integration.py  # Komponenty razem
‚îî‚îÄ‚îÄ test_bash_execution.py         # Framework + prawdziwy bash

# 2. Docker test environment
development/docker_test/
‚îú‚îÄ‚îÄ test_containers.yml            # Test-specific containers
‚îî‚îÄ‚îÄ integration_test_runner.sh     # Integration test launcher
```

### **Faza 3: E2E & Performance (Tydzie≈Ñ 4)**

```python
# 1. E2E testy w Docker
tests/e2e/
‚îú‚îÄ‚îÄ test_docker_integration.py     # Framework w kontenerach
‚îú‚îÄ‚îÄ test_ssh_connectivity.py       # SSH miƒôdzy kontenerami
‚îî‚îÄ‚îÄ test_performance.py            # Performance metrics

# 2. Monitoring i reporting
tests/reports/
‚îú‚îÄ‚îÄ coverage_report.py             # Test coverage analysis
‚îî‚îÄ‚îÄ performance_benchmarks.py      # Performance tracking
```

### **Faza 4: Automation & Monitoring (Tydzie≈Ñ 5)**

```python
# 1. Automated test execution
scripts/
‚îú‚îÄ‚îÄ run_all_tests.sh               # Master test runner
‚îú‚îÄ‚îÄ run_quick_tests.sh             # Smoke tests dla dev
‚îî‚îÄ‚îÄ run_performance_tests.sh       # Performance testing

# 2. Quality gates
.pre-commit-hooks.yaml             # Pre-commit test execution
pytest.ini                        # Pytest configuration
coverage.ini                      # Coverage requirements
```

---

## üéØ **Podsumowanie strategii testowania**

### **Co framework Mancer MUSI mieƒá:**

1. **‚úÖ Unit Tests (70%)** - CommandFactory, BashBackend, ShellRunner
2. **‚úÖ Integration Tests (20%)** - Framework + bash execution  
3. **‚úÖ E2E Tests (10%)** - Framework w Docker + SSH
4. **‚úÖ Performance Tests** - Cache, execution time, memory usage
5. **‚úÖ Error Handling Tests** - Graceful failures, timeouts
6. **‚úÖ Automated CI/CD** - Tests na ka≈ºdy commit
7. **‚úÖ Regression Tests** - Backward compatibility

### **Dlaczego to jest konieczne:**

- **Framework = Foundation** - B≈Çƒôdy w frameworku = broken applications
- **Bash Wrapper = Complex** - Wiele edge cases, environment dependencies  
- **Multiple Backends** - SSH, bash, powershell - wszystko musi dzia≈Çaƒá
- **Performance Critical** - Framework nie mo≈ºe byƒá bottleneck
- **Developer Experience** - Dobry framework = ≈Çatwe u≈ºycie

**System test√≥w to inwestycja w przysz≈Ço≈õƒá frameworka!** üöÄ 